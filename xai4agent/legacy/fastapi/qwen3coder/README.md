# FastAPI OpenAI-Compatible Server for Qwen3-Coder-30B

ä¸€ä¸ªåŸºäº FastAPI + Transformers çš„ OpenAI å…¼å®¹æœåŠ¡å™¨ï¼Œä¸“ä¸º qwen-code è®¾è®¡ã€‚

## ç›®å½•

- [å¿«é€Ÿå¼€å§‹](#å¿«é€Ÿå¼€å§‹)
- [åŠŸèƒ½ç‰¹æ€§](#åŠŸèƒ½ç‰¹æ€§)
- [æ€§èƒ½å¯¹æ¯”](#æ€§èƒ½å¯¹æ¯”)
- [é…ç½®è¯´æ˜](#é…ç½®è¯´æ˜)
- [API ä½¿ç”¨](#api-ä½¿ç”¨)
- [qwen-code é›†æˆ](#qwen-code-é›†æˆ)
- [æ•…éšœæ’æŸ¥](#æ•…éšœæ’æŸ¥)
- [vLLM åŸç”Ÿäº¤äº’å‚è€ƒ](#vllm-åŸç”Ÿäº¤äº’å‚è€ƒ)
- [å¼€å‘å†å²](#å¼€å‘å†å²)
- [æŠ€æœ¯æ¶æ„](#æŠ€æœ¯æ¶æ„)

---

## å¿«é€Ÿå¼€å§‹

### å¯åŠ¨æœåŠ¡å™¨

```bash
cd /root/autodl-tmp/FastAPI/qwen3coder

# æ–¹å¼ 1: ä½¿ç”¨å¿«é€Ÿå¯åŠ¨è„šæœ¬ï¼ˆæ¨èï¼‰
./quick_start.sh

# æ–¹å¼ 2: åå°è¿è¡Œ
nohup python server.py > server.log 2>&1 &

# æ–¹å¼ 3: ç›´æ¥è¿è¡Œ
python server.py
```

### éªŒè¯è¿è¡Œ

```bash
# æµ‹è¯•æ¨¡å‹åˆ—è¡¨
curl http://127.0.0.1:8000/v1/models

# è¿è¡Œå®Œæ•´æµ‹è¯•
python test_client.py
```

### æŸ¥çœ‹æ—¥å¿—

```bash
# å®æ—¶æŸ¥çœ‹æ—¥å¿—
tail -f server.log

# æŸ¥çœ‹æœ€è¿‘é”™è¯¯
tail -50 server.log | grep ERROR
```

### åœæ­¢æœåŠ¡å™¨

```bash
ps aux | grep "python server.py" | grep -v grep | awk '{print $2}' | xargs -r kill
```

---

## åŠŸèƒ½ç‰¹æ€§

### âœ… å·²å®ç°

| åŠŸèƒ½ | çŠ¶æ€ | è¯´æ˜ |
|------|------|------|
| **GET /v1/models** | âœ… | åˆ—å‡ºå¯ç”¨æ¨¡å‹ |
| **POST /v1/chat/completions** | âœ… | èŠå¤©è¡¥å…¨ API |
| **éæµå¼å“åº”** | âœ… | æ ‡å‡†å®Œæ•´å“åº” |
| **æµå¼å“åº”** | âœ… | SSE æ ¼å¼æµå¼è¾“å‡º |
| **å·¥å…·è°ƒç”¨** | âœ… | ä½¿ç”¨ vLLM qwen3_xml è§£æå™¨ |
| **tool_choice="auto"** | âœ… | è‡ªåŠ¨å·¥å…·è°ƒç”¨æ”¯æŒ |
| **ç‰¹æ®Š token è¿‡æ»¤** | âœ… | è¿‡æ»¤ `<|im_start|>` ç­‰ |
| **å¢é‡ decode** | âœ… | TokenAccumulator é¿å…ä¹±ç  |
| **è¯·æ±‚æ—¥å¿—** | âœ… | å®Œæ•´è¯·æ±‚ä½“è®°å½• |

### ğŸ”§ æŠ€æœ¯æ ˆ

- **Python 3.12+**
- **FastAPI** - Web æ¡†æ¶
- **Transformers** - æ¨¡å‹åŠ è½½
- **PyTorch** - æ·±åº¦å­¦ä¹ æ¡†æ¶
- **vLLM Tool Parser** - qwen3_xml å·¥å…·è§£æ

### âš™ï¸ é»˜è®¤é…ç½®

```python
MODEL_PATH = "/root/autodl-tmp/models/qwen3-coder-30b"  # 57GB bf16
MODEL_NAME = "qwen3-coder-30b"
MAX_MODEL_LEN = 163840
HOST = "127.0.0.1"
PORT = 8000
```

---

## æ€§èƒ½å¯¹æ¯”

### ä¸ vLLM å¯¹æ¯”

| æŒ‡æ ‡ | vLLM | FastAPI Server | è¯´æ˜ |
|------|------|----------------|------|
| **æ¨¡å‹** | qwen3-coder-30b (bf16) | qwen3-coder-30b (bf16) | å®Œå…¨ç›¸åŒ |
| **å†…å­˜å ç”¨** | ~80 GB | ~57 GB | æ›´ä½å†…å­˜ |
| **Prompt ååé‡** | 1180 tok/s | ~50-100 tok/s | æ…¢ 10-20 å€ |
| **Generation ååé‡** | 24.6 tok/s | ~10-20 tok/s | æ…¢ 2-3 å€ |
| **å¹¶å‘æ”¯æŒ** | âœ… æ”¯æŒ | âŒ å•è¯·æ±‚ | å¼€å‘ç¯å¢ƒé™åˆ¶ |
| **max_model_len** | 163840 | 163840 | å®Œå…¨ç›¸åŒ |
| **tool_call_parser** | qwen3_xml | qwen3_xml | å¤ç”¨ vLLM |
| **çœŸæµå¼ç”Ÿæˆ** | âœ… | âŒ ä¼ªæµå¼ | Transformers é™åˆ¶ |

### é€‚ç”¨åœºæ™¯

**æœ¬æœåŠ¡å™¨é€‚ç”¨äºï¼š**
- âœ… å•ç”¨æˆ·å¼€å‘/è°ƒè¯•
- âœ… åŠŸèƒ½éªŒè¯å’Œæµ‹è¯•
- âœ… å†…å­˜å—é™ç¯å¢ƒ
- âœ… å¿«é€ŸåŸå‹å¼€å‘

**vLLM é€‚ç”¨äºï¼š**
- âœ… ç”Ÿäº§ç¯å¢ƒé«˜å¹¶å‘
- âœ… é«˜ååé‡éœ€æ±‚
- âœ… å¤šç”¨æˆ·æœåŠ¡

---

## é…ç½®è¯´æ˜

### æ¨¡å‹è·¯å¾„

ç¼–è¾‘ `server.py` ä¸­çš„ `MODEL_PATH`ï¼š

```python
# ä½¿ç”¨ bf16 æ¨¡å‹ï¼ˆ57GBï¼Œæ¨èï¼‰
MODEL_PATH = "/root/autodl-tmp/models/qwen3-coder-30b"

# æˆ–ä½¿ç”¨ fp8 é‡åŒ–æ¨¡å‹ï¼ˆ30GBï¼Œéœ€è¦ä¿®æ”¹ dtypeï¼‰
MODEL_PATH = "/root/autodl-tmp/models/qwen3-coder-30b-fp8"
```

### æœåŠ¡å™¨è®¾ç½®

```bash
# è‡ªå®šä¹‰ host/port
python server.py --host 0.0.0.0 --port 8080

# æˆ–ä¿®æ”¹ server.py ä¸­çš„é…ç½®
HOST = "0.0.0.0"  # å…è®¸å¤–éƒ¨è®¿é—®
PORT = 8080
```

### ç¯å¢ƒå˜é‡

```bash
# è®¾ç½® PYTHONPATH ä»¥å¯¼å…¥ vLLM
export PYTHONPATH="/root/miniconda3/envs/qwen/lib/python3.12/site-packages:$PYTHONPATH"
```

---

## API ä½¿ç”¨

### Python SDK

#### å®‰è£…

```bash
pip install openai
```

#### åŸºæœ¬å¯¹è¯

```python
from openai import OpenAI

client = OpenAI(
    base_url="http://127.0.0.1:8000/v1",
    api_key="dummy"  # ä¸å®é™…éªŒè¯
)

response = client.chat.completions.create(
    model="qwen3-coder-30b",
    messages=[{"role": "user", "content": "What is 2+2?"}]
)

print(response.choices[0].message.content)
# è¾“å‡º: 2+2 equals 4.
```

#### æµå¼è¾“å‡º

```python
stream = client.chat.completions.create(
    model="qwen3-coder-30b",
    messages=[{"role": "user", "content": "Count from 1 to 10"}],
    stream=True
)

for chunk in stream:
    if chunk.choices[0].delta.content:
        print(chunk.choices[0].delta.content, end="", flush=True)
# è¾“å‡º: 1, 2, 3, 4, 5, 6, 7, 8, 9, 10
```

#### å·¥å…·è°ƒç”¨

```python
tools = [{
    "type": "function",
    "function": {
        "name": "get_weather",
        "description": "Get the current weather for a city",
        "parameters": {
            "type": "object",
            "properties": {
                "city": {
                    "type": "string",
                    "description": "The city name"
                }
            },
            "required": ["city"]
        }
    }
}]

response = client.chat.completions.create(
    model="qwen3-coder-30b",
    messages=[{"role": "user", "content": "What's the weather in Beijing?"}],
    tools=tools
)

if response.choices[0].message.tool_calls:
    for tc in response.choices[0].message.tool_calls:
        print(f"Tool: {tc.function.name}")
        print(f"Args: {tc.function.arguments}")
        # è¾“å‡º:
        # Tool: get_weather
        # Args: {"city":"Beijing"}
```

#### å¤šè½®å¯¹è¯

```python
messages = [
    {"role": "system", "content": "You are a helpful assistant."},
    {"role": "user", "content": "Remember my favorite color: blue"}
]

response = client.chat.completions.create(
    model="qwen3-coder-30b",
    messages=messages
)

messages.append({"role": "assistant", "content": response.choices[0].message.content})
messages.append({"role": "user", "content": "What's my favorite color?"})

response = client.chat.completions.create(
    model="qwen3-coder-30b",
    messages=messages
)

print(response.choices[0].message.content)
# è¾“å‡º: Your favorite color is blue.
```

### cURL

#### éæµå¼è¯·æ±‚

```bash
curl http://127.0.0.1:8000/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "qwen3-coder-30b",
    "messages": [{"role": "user", "content": "Hello!"}]
  }'
```

#### æµå¼è¯·æ±‚

```bash
curl http://127.0.0.1:8000/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "qwen3-coder-30b",
    "messages": [{"role": "user", "content": "Count to 5"}],
    "stream": true
  }'
```

#### å·¥å…·è°ƒç”¨

```bash
curl http://127.0.0.1:8000/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "qwen3-coder-30b",
    "messages": [{"role": "user", "content": "What is the weather in Beijing?"}],
    "tools": [{
      "type": "function",
      "function": {
        "name": "get_weather",
        "description": "Get weather for a city",
        "parameters": {
          "type": "object",
          "properties": {
            "city": {"type": "string"}
          }
        }
      }
    }]
  }'
```

### API å‚è€ƒ

#### GET /v1/models

åˆ—å‡ºå¯ç”¨æ¨¡å‹ã€‚

**å“åº”ç¤ºä¾‹ï¼š**
```json
{
  "object": "list",
  "data": [{
    "id": "qwen3-coder-30b",
    "object": "model",
    "created": 1234567890,
    "owned_by": "qwen"
  }]
}
```

#### POST /v1/chat/completions

åˆ›å»ºèŠå¤©è¡¥å…¨ã€‚

**è¯·æ±‚å‚æ•°ï¼š**
| å‚æ•° | ç±»å‹ | å¿…å¡« | è¯´æ˜ |
|------|------|------|------|
| model | string | âœ… | æ¨¡å‹åç§° |
| messages | array | âœ… | å¯¹è¯æ¶ˆæ¯åˆ—è¡¨ |
| stream | boolean | âŒ | æ˜¯å¦æµå¼è¾“å‡ºï¼ˆé»˜è®¤ falseï¼‰|
| temperature | number | âŒ | é‡‡æ ·æ¸©åº¦ï¼ˆ0-2ï¼‰|
| max_tokens | integer | âŒ | æœ€å¤§ç”Ÿæˆ token æ•° |
| tools | array | âŒ | å·¥å…·å®šä¹‰åˆ—è¡¨ |
| tool_choice | string/object | âŒ | å·¥å…·é€‰æ‹©ç­–ç•¥ï¼ˆé»˜è®¤ "auto"ï¼‰|

**æ¶ˆæ¯æ ¼å¼ï¼š**
```json
{
  "role": "user|assistant|system|tool",
  "content": "string or array",
  "tool_calls": [...],  // assistant æ¶ˆæ¯å¯é€‰
  "tool_call_id": "string"  // tool æ¶ˆæ¯å¿…éœ€
}
```

---

## qwen-code é›†æˆ

### é…ç½® settings.json

åœ¨ `/root/.qwen/settings.json` ä¸­é…ç½®ï¼š

```json
{
  "security": {
    "auth": {
      "selectedType": "openai",
      "apiKey": "EMPTY",
      "baseUrl": "http://127.0.0.1:8000/v1"
    }
  },
  "model": {
    "name": "qwen3-coder-30b",
    "enableOpenAILogging": true,
    "openAILoggingDir": "/root/autodl-tmp/tmp"
  }
}
```

### æ—¥å¿—ä½ç½®

qwen-code çš„è¯·æ±‚æ—¥å¿—ä¿å­˜åœ¨ï¼š
```
/root/autodl-tmp/tmp/openai-*.json
```

æŸ¥çœ‹æœ€æ–°æ—¥å¿—ï¼š
```bash
ls -lt /root/autodl-tmp/tmp/openai-*.json | head -1
```

---

## æ•…éšœæ’æŸ¥

### å¸¸è§é—®é¢˜

#### 1. å†…å­˜ä¸è¶³ (OOM)

**é”™è¯¯ä¿¡æ¯ï¼š**
```
CUDA out of memory
```

**è§£å†³æ–¹æ¡ˆï¼š**
- ä½¿ç”¨ fp8 é‡åŒ–æ¨¡å‹ï¼ˆ30GB vs 57GBï¼‰
- é™ä½ `max_tokens` å‚æ•°
- å‡å°‘è¾“å…¥æ¶ˆæ¯é•¿åº¦
- æ¸…ç† GPU ç¼“å­˜ï¼š`torch.cuda.empty_cache()`

#### 2. accelerate åº“é”™è¯¯

**é”™è¯¯ä¿¡æ¯ï¼š**
```
ValueError: Using a device_map requires accelerate
```

**è§£å†³æ–¹æ¡ˆï¼š**
- å·²ä¿®å¤ï¼šä½¿ç”¨ `.to("cuda")` æ›¿ä»£ `device_map="auto"`
- æ— éœ€å®‰è£… accelerate

#### 3. torch_dtype å¼ƒç”¨è­¦å‘Š

**é”™è¯¯ä¿¡æ¯ï¼š**
```
torch_dtype is deprecated! Use dtype instead
```

**è§£å†³æ–¹æ¡ˆï¼š**
- å·²ä¿®å¤ï¼šæ”¹ç”¨ `dtype=torch.bfloat16`

#### 4. TypeError: Can only get item pairs from a mapping

**é”™è¯¯ä¿¡æ¯ï¼š**
```
TypeError: Can only get item pairs from a mapping
  at "<template>", line 87, in top-level template code
```

**åŸå› ï¼š** å¤šç§å¯èƒ½åŸå› 
1. Pydantic æ¨¡å‹æœªå®Œå…¨è½¬æ¢ä¸º dict
2. `tool_call.arguments` æ˜¯ JSON å­—ç¬¦ä¸²ä½†æ¨¡æ¿æœŸæœ› dict

**è§£å†³æ–¹æ¡ˆï¼š**
- å·²ä¿®å¤ï¼šé€’å½’è½¬æ¢ Pydantic æ¨¡å‹ä¸º dict
- å·²ä¿®å¤ï¼šæ·»åŠ  `_postprocess_tool_calls()` å°† arguments ä» JSON å­—ç¬¦ä¸²è½¬ä¸º dict

#### 5. æµå¼å·¥å…·è°ƒç”¨ terminated

**é”™è¯¯ä¿¡æ¯ï¼š**
```
OpenAI API Streaming Error: terminated
```

**åŸå› ï¼š** æ¯ä¸ª token ç”Ÿæˆæ–°çš„ tool_call_id

**è§£å†³æ–¹æ¡ˆï¼š**
- å·²ä¿®å¤ï¼šæ·»åŠ  `current_tool_call_id` å˜é‡ä¿æŒä¸€è‡´æ€§

#### 6. å·¥å…·è°ƒç”¨ä¸å·¥ä½œ

**å¯èƒ½åŸå› ï¼š**
- vLLM æœªæ­£ç¡®å®‰è£…
- PYTHONPATH æœªè®¾ç½®
- å·¥å…·å®šä¹‰æ ¼å¼é”™è¯¯

**è§£å†³æ–¹æ¡ˆï¼š**
```bash
# æ£€æŸ¥ vLLM å®‰è£…
python -c "import vllm; print(vllm.__version__)"

# è®¾ç½® PYTHONPATH
export PYTHONPATH="/root/miniconda3/envs/qwen/lib/python3.12/site-packages:$PYTHONPATH"

# æ£€æŸ¥å·¥å…·å®šä¹‰æ ¼å¼
# ç¡®ä¿ parameters æ˜¯æœ‰æ•ˆçš„ JSON Schema
```

#### 7. å·¥å…·å‚æ•°ç±»å‹é”™è¯¯ï¼ˆæ•°ç»„å˜å­—ç¬¦ä¸²ï¼‰

**ç—‡çŠ¶ï¼š** `{"todos":"[...]"}` è¢«è§£æä¸ºå­—ç¬¦ä¸²ï¼Œè€Œä¸æ˜¯æ•°ç»„ã€‚  
**åŸå› ï¼š** å·¥å…·è§£æå™¨æ²¡æœ‰è¯»å–åˆ°å‚æ•°ç±»å‹ï¼Œå…¨éƒ¨é€€åŒ–ä¸º stringã€‚  
**è§£å†³æ–¹æ¡ˆï¼š** ç¡®ä¿ vLLM çš„å·¥å…·è§£æå™¨æ¥æ”¶åˆ° Pydantic å·¥å…·å¯¹è±¡ï¼ˆè€Œé dictï¼‰ï¼Œå¹¶ä½¿ç”¨è§£æå™¨è¾“å‡ºçš„æµå¼å†…å®¹ï¼Œé¿å… `<tool_call>` æ³„éœ²ã€‚

### æ—¥å¿—è°ƒè¯•

#### æŸ¥çœ‹æœåŠ¡å™¨æ—¥å¿—

```bash
# å®æ—¶æ—¥å¿—
tail -f server.log

# æŸ¥çœ‹æœ€è¿‘ 50 è¡Œ
tail -50 server.log

# åªçœ‹é”™è¯¯
grep ERROR server.log

# æŸ¥çœ‹å®Œæ•´è¯·æ±‚
grep "Request:" server.log | tail -10
```

#### æŸ¥çœ‹ qwen-code æ—¥å¿—

```bash
# æŸ¥çœ‹æœ€æ–°çš„ OpenAI è¯·æ±‚æ—¥å¿—
ls -lt /root/autodl-tmp/tmp/openai-*.json | head -1 | xargs cat

# æŸ¥çœ‹æœ€è¿‘çš„ 5 ä¸ªè¯·æ±‚
ls -lt /root/autodl-tmp/tmp/openai-*.json | head -5 | xargs -I {} sh -c 'echo "=== {} ===" && cat {}'
```

### æ€§èƒ½ä¼˜åŒ–

#### å½“å‰é™åˆ¶

- **å•è¯·æ±‚å¤„ç†** - ä¸æ”¯æŒå¹¶å‘
- **ä¼ªæµå¼** - ç”Ÿæˆå®Œååˆ†å—å‘é€ï¼ŒéçœŸæµå¼
- **æ— ç¼“å­˜** - æ¯æ¬¡è¯·æ±‚éƒ½é‡æ–°å¤„ç†

#### å¯é€‰ä¼˜åŒ–

##### 1. ä½¿ç”¨ vLLMï¼ˆç”Ÿäº§ç¯å¢ƒæ¨èï¼‰

```bash
python -m vllm.entrypoints.openai.api_server \
    --model /root/autodl-tmp/models/qwen3-coder-30b \
    --host 127.0.0.1 --port 8000 \
    --served-model-name qwen3-coder-30b \
    --max-model-len 163840 \
    --enable-auto-tool-choice \
    --tool-call-parser qwen3_xml
```

##### 2. æ¨¡å‹é‡åŒ–

```python
from transformers import BitsAndBytesConfig

quantization_config = BitsAndBytesConfig(load_in_8bit=True)
model = AutoModelForCausalLM.from_pretrained(
    MODEL_PATH,
    quantization_config=quantization_config,
    ...
)
```

##### 3. æ·»åŠ ç¼“å­˜å±‚

å¯¹å¸¸è§è¯·æ±‚ç¼“å­˜ç»“æœä»¥æå‡å“åº”é€Ÿåº¦ã€‚

---

## vLLM åŸç”Ÿäº¤äº’å‚è€ƒ

### é¡¹ç›®ç›®æ ‡

æœ¬é¡¹ç›®çš„æ ¸å¿ƒç›®æ ‡æ˜¯ **ç”¨ Transformers + FastAPI æ›¿ä»£ vLLM** æ¥åŠ è½½ Qwen3-Coder æ¨¡å‹ï¼ŒåŒæ—¶ä¿æŒä¸ qwen-code çš„å®Œå…¨å…¼å®¹æ€§ã€‚

**ä¸ºä»€ä¹ˆæ›¿ä»£ vLLMï¼Ÿ**
- âœ… æ›´ä½çš„å†…å­˜å ç”¨ï¼ˆ57GB vs 80GBï¼‰
- âœ… æ›´ç®€å•çš„ä¾èµ–å…³ç³»ï¼ˆä¸éœ€è¦å®Œæ•´çš„ vLLMï¼‰
- âœ… æ›´æ˜“äºè°ƒè¯•å’Œå®šåˆ¶
- âœ… é€‚åˆå•ç”¨æˆ·å¼€å‘ç¯å¢ƒ

**ä¿ç•™ä»€ä¹ˆï¼Ÿ**
- âœ… å®Œå…¨å¤ç”¨ vLLM çš„ qwen3_xml å·¥å…·è§£æå™¨
- âœ… å®Œå…¨å…¼å®¹ OpenAI API æ ¼å¼
- âœ… ç›¸åŒçš„ chat template å¤„ç†
- âœ… ç›¸åŒçš„æ¨¡å‹è¾“å‡º

### vLLM å®ç°è§£æ

#### 1. æ¨¡å‹æ³¨å†Œæœºåˆ¶

vLLM é€šè¿‡ `ModelRegistry` ç®¡ç†æ‰€æœ‰æ¨¡å‹æ¶æ„ï¼š

**vLLM æºç ï¼š**
```python
# /vllm/model_executor/models/registry.py
_VLLM_MODELS = {
    "Qwen3ForCausalLM": ("qwen3", "Qwen3ForCausalLM"),
    "Qwen3MoeForCausalLM": ("qwen3_moe", "Qwen3MoeForCausalLM"),
}
```

**æˆ‘ä»¬çš„å®ç°ï¼š**
```python
# server.py
from transformers import AutoModelForCausalLM

model = AutoModelForCausalLM.from_pretrained(
    MODEL_PATH,
    dtype=torch.bfloat16,
    trust_remote_code=True,
)
model = model.to("cuda")  # æ‰‹åŠ¨è®¾å¤‡æ˜ å°„
```

#### 2. Qwen3 æ¨¡å‹æ¶æ„

**vLLM æºç ï¼š**
```python
# /vllm/model_executor/models/qwen3.py
class Qwen3Model(Qwen2Model):
    def __init__(self, *, vllm_config: VllmConfig, prefix: str = ""):
        super().__init__(
            vllm_config=vllm_config,
            prefix=prefix,
            decoder_layer_type=Qwen3DecoderLayer
        )

class Qwen3ForCausalLM(nn.Module, SupportsLoRA, SupportsPP):
    packed_modules_mapping = {
        "qkv_proj": ["q_proj", "k_proj", "v_proj"],
        "gate_up_proj": ["gate_proj", "up_proj"],
    }
```

**å…³é”®ç‰¹æ€§ï¼š**
- ç»§æ‰¿è‡ª Qwen2Model
- æ”¯æŒ Packed attentionï¼ˆQKV åˆå¹¶ï¼‰
- æ”¯æŒ Pipeline Parallelismï¼ˆPPï¼‰
- æ”¯æŒ LoRA

**æˆ‘ä»¬çš„å®ç°ï¼š** ä½¿ç”¨æ ‡å‡† Transformers åŠ è½½ï¼Œè‡ªåŠ¨æ”¯æŒè¿™äº›ç‰¹æ€§ã€‚

#### 3. qwen3_xml å·¥å…·è§£æå™¨

vLLM çš„å·¥å…·è§£æå™¨æ˜¯æ ¸å¿ƒç»„ä»¶ï¼Œæˆ‘ä»¬**å®Œå…¨å¤ç”¨**ï¼š

**vLLM æºç ï¼š**
```python
# /vllm/entrypoints/openai/tool_parsers/qwen3xml_tool_parser.py

class StreamingXMLToolCallParser:
    """æµå¼ XML å·¥å…·è°ƒç”¨è§£æå™¨"""

    # XML æ ‡è®°å®šä¹‰
    tool_call_start_token = "<|tool_call|>"
    tool_call_end_token = "<|end_tool_call|>"
    function_start_token = "<function="
    function_end_token = "</function>"
    # ... æ›´å¤šå®ç°
```

**æˆ‘ä»¬çš„å®ç°ï¼ˆå®Œå…¨å¤ç”¨ï¼‰ï¼š**
```python
# server.py - ä» vLLM å®Œå…¨å¤ç”¨
from vllm.entrypoints.openai.tool_parsers.qwen3xml_tool_parser import (
    StreamingXMLToolCallParser
)

tool_parser = StreamingXMLToolCallParser()
tool_parser.set_tools(request.tools)

# æµå¼è§£æ
result = tool_parser.parse_single_streaming_chunks(delta_text)
```

**å…³é”®åŠŸèƒ½ï¼š**
- è§£æ Qwen3 çš„ XML æ ¼å¼å·¥å…·è°ƒç”¨
- æ”¯æŒæµå¼å¢é‡è§£æ
- è‡ªåŠ¨å¤„ç†å‚æ•°ç±»å‹è½¬æ¢
- å®¹é”™å¤„ç†ä¸å®Œæ•´çš„ XML

#### 4. Chat Template å¤„ç†

**vLLM å®ç°ï¼š**
```python
# /vllm/entrypoints/chat_utils.py
from transformers import PreTrainedTokenizer

# ä½¿ç”¨ tokenizer çš„ chat template
prompt = tokenizer.apply_chat_template(
    messages,
    tools=tools,
    add_generation_prompt=True,
    tokenize=False,
    return_dict=False
)
```

**æˆ‘ä»¬çš„å®ç°ï¼š**
```python
# server.py - å®Œå…¨ç›¸åŒ
prompt = tokenizer.apply_chat_template(
    formatted_messages,  # é€’å½’è½¬æ¢åçš„ messages
    tools=formatted_tools,  # é€’å½’è½¬æ¢åçš„ tools
    add_generation_prompt=True,
    tokenize=False,
    return_dict=False
)
```

**å…³é”®è½¬æ¢ï¼š**
```python
def format_messages_for_template(messages: list[ChatMessage]) -> list[dict]:
    """é€’å½’è½¬æ¢ Pydantic æ¨¡å‹ä¸º dictï¼Œé€‚é… Jinja2 æ¨¡æ¿"""
    formatted = []
    for msg in messages:
        # è½¬æ¢ä¸º dict
        if hasattr(msg, "model_dump"):
            msg_dict = msg.model_dump(exclude_none=True)
        else:
            msg_dict = dict(msg)
        
        # é€’å½’å¤„ç† tool_calls
        if msg_dict.get("tool_calls"):
            formatted_tool_calls = []
            for tc in msg_dict["tool_calls"]:
                if hasattr(tc, "model_dump"):
                    tc_dict = tc.model_dump(exclude_none=True)
                formatted_tool_calls.append(tc_dict)
            msg_dict["tool_calls"] = formatted_tool_calls
        
        formatted.append(msg_dict)
    return formatted
```

### vLLM ä¸æœ¬æœåŠ¡å¯¹æ¯”

| ç»„ä»¶ | vLLM | æœ¬æœåŠ¡ | è¯´æ˜ |
|------|------|--------|------|
| **æ¨¡å‹åŠ è½½** | ModelRegistry + è‡ªå®šä¹‰ç±» | AutoModelForCausalLM | Transformers æ ‡å‡† API |
| **è®¾å¤‡æ˜ å°„** | device_map="auto" | .to("cuda") | é¿å…ä¾èµ– accelerate |
| **dtype å‚æ•°** | torch_dtype | dtype | é€‚é…æ–°ç‰ˆæœ¬ Transformers |
| **å·¥å…·è§£æå™¨** | StreamingXMLToolCallParser | å®Œå…¨å¤ç”¨ | ä» vLLM å¯¼å…¥ |
| **Chat Template** | apply_chat_template | å®Œå…¨ç›¸åŒ | Transformers å†…ç½® |
| **æµå¼ç”Ÿæˆ** | çœŸÂ·æµå¼ï¼ˆKV Cacheï¼‰ | ä¼ªæµå¼ï¼ˆç”Ÿæˆååˆ†å—ï¼‰| Transformers é™åˆ¶ |
| **å¹¶å‘å¤„ç†** | æ”¯æŒï¼ˆAsyncLLMEngineï¼‰ | ä¸æ”¯æŒ | å•ç”¨æˆ·ç¯å¢ƒ |

### å‚è€ƒå®ç°è·¯å¾„

**vLLM æºç ä½ç½®ï¼š**
```
/root/miniconda3/envs/qwen/lib/python3.12/site-packages/vllm/
â”œâ”€â”€ model_executor/models/
â”‚   â”œâ”€â”€ qwen3.py                    # Qwen3 æ¨¡å‹å®ç°
â”‚   â””â”€â”€ registry.py                 # æ¨¡å‹æ³¨å†Œ
â”œâ”€â”€ entrypoints/
â”‚   â”œâ”€â”€ openai/
â”‚   â”‚   â””â”€â”€ tool_parsers/
â”‚   â”‚       â””â”€â”€ qwen3xml_tool_parser.py  # å·¥å…·è§£æå™¨
â”‚   â””â”€â”€ chat_utils.py               # Chat å·¥å…·å‡½æ•°
â””â”€â”€ transformers_utils/
    â””â”€â”€ tokenizer.py                # Tokenizer åŒ…è£…
```

**å…³é”®ä»£ç ç‰‡æ®µï¼š**

1. **æ¨¡å‹æ³¨å†Œ** - `registry.py:163-164`
2. **Qwen3 ç±»** - `qwen3.py:258-299`
3. **å·¥å…·è§£æå™¨** - `qwen3xml_tool_parser.py:31-1317`
4. **Chat Template** - `chat_utils.py:1-100`

### å…¼å®¹æ€§éªŒè¯

æœ¬æœåŠ¡å™¨å·²é€šè¿‡ä»¥ä¸‹å…¼å®¹æ€§æµ‹è¯•ï¼š

| æµ‹è¯•é¡¹ | vLLM è¡Œä¸º | æœ¬æœåŠ¡å™¨ | ç»“æœ |
|--------|-----------|----------|------|
| å·¥å…·è°ƒç”¨æ ¼å¼ | XML | XMLï¼ˆå¤ç”¨è§£æå™¨ï¼‰ | âœ… å®Œå…¨ç›¸åŒ |
| æµå¼è¾“å‡º | SSE | SSEï¼ˆä¼ªæµå¼ï¼‰ | âœ… æ ¼å¼å…¼å®¹ |
| éæµå¼è¾“å‡º | JSON | JSON | âœ… å®Œå…¨ç›¸åŒ |
| Chat Template | Jinja2 | Jinja2 | âœ… å®Œå…¨ç›¸åŒ |
| tool_choice="auto" | æ”¯æŒ | æ”¯æŒ | âœ… å®Œå…¨ç›¸åŒ |
| å¤šè½®å¯¹è¯ | æ”¯æŒ | æ”¯æŒ | âœ… å®Œå…¨ç›¸åŒ |
| ç‰¹æ®Š token è¿‡æ»¤ | è‡ªåŠ¨ | æ‰‹åŠ¨è¿‡æ»¤ | âœ… æ•ˆæœç›¸åŒ |

---

## å¼€å‘å†å²

### ä¿®å¤è®°å½•

#### ä¿®å¤ 1: device_map éœ€è¦ accelerate
**é—®é¢˜ï¼š** `ValueError: Using a device_map requires accelerate`
**è§£å†³ï¼š** æ”¹ç”¨ `.to("cuda")` æ‰‹åŠ¨è®¾å¤‡æ˜ å°„
**æ–‡ä»¶ï¼š** server.py:238
**æ—¥æœŸï¼š** 2026-01-11

#### ä¿®å¤ 2: fp8 æ¨¡å‹éœ€è¦ accelerate
**é—®é¢˜ï¼š** `ImportError: Loading an FP8 quantized model requires accelerate`
**è§£å†³ï¼š** æ”¹ç”¨ bf16 æ¨¡å‹ `qwen3-coder-30b`ï¼ˆ57GBï¼‰
**æ–‡ä»¶ï¼š** server.py:25
**æ—¥æœŸï¼š** 2026-01-11

#### ä¿®å¤ 3: torch_dtype deprecated
**é—®é¢˜ï¼š** `torch_dtype is deprecated! Use dtype instead`
**è§£å†³ï¼š** æ”¹ç”¨ `dtype=torch.bfloat16`
**æ–‡ä»¶ï¼š** server.py:235
**æ—¥æœŸï¼š** 2026-01-11

#### ä¿®å¤ 4: tool_call_id ä¸ä¸€è‡´
**é—®é¢˜ï¼š** æµå¼å·¥å…·è°ƒç”¨æ¯ä¸ª token ç”Ÿæˆæ–° ID
**è§£å†³ï¼š** æ·»åŠ  `current_tool_call_id` å˜é‡ä¿æŒä¸€è‡´æ€§
**æ–‡ä»¶ï¼š** server.py:549, 575-576
**æ—¥æœŸï¼š** 2026-01-11

#### ä¿®å¤ 5: Pydantic æ¨¡å‹æœªè½¬æ¢ä¸º dict
**é—®é¢˜ï¼š** `TypeError: Can only get item pairs from a mapping`
**è§£å†³ï¼š**
- é€’å½’è½¬æ¢ `format_tools_for_template` ä¸­çš„ parameters
- é€’å½’è½¬æ¢ `format_messages_for_template` ä¸­çš„ tool_calls
- ä½¿ç”¨ `model_dump(exclude_none=True)` å®Œå…¨è½¬æ¢
**æ–‡ä»¶ï¼š** server.py:320-341, 286-335
**æ—¥æœŸï¼š** 2026-01-11

#### ä¿®å¤ 6: tool_call.arguments ç±»å‹ä¸åŒ¹é…
**é—®é¢˜ï¼š** qwen3 èŠå¤©æ¨¡æ¿æœŸæœ› `tool_call.arguments` ä¸º dictï¼Œä½† OpenAI æ ¼å¼ä¸º JSON å­—ç¬¦ä¸²
**é”™è¯¯ä¿¡æ¯ï¼š**
```
TypeError: Can only get item pairs from a mapping
  at "<template>", line 87, in top-level template code
  {%- for args_name, args_value in tool_call.arguments|items %}
```
**æ ¹æœ¬åŸå› ï¼š**
- qwen code å‘é€åŒ…å«ä¹‹å‰å·¥å…·è°ƒç”¨å†å²çš„è¯·æ±‚æ—¶ï¼Œ`arguments` æ˜¯ JSON å­—ç¬¦ä¸²ï¼ˆOpenAI æ ¼å¼ï¼‰
- qwen3 èŠå¤©æ¨¡æ¿ä½¿ç”¨ `tool_call.arguments|items` éå†å‚æ•°ï¼ŒæœŸæœ› dict ç±»å‹
- å½“æ¨¡æ¿å°è¯•å¯¹å­—ç¬¦ä¸²è°ƒç”¨ `.items()` æ—¶å¤±è´¥
**è§£å†³ï¼š**
- æ·»åŠ  `_postprocess_tool_calls()` å‡½æ•°ï¼Œåœ¨è°ƒç”¨ `apply_chat_template()` å‰è½¬æ¢
- å°† assistant æ¶ˆæ¯ä¸­çš„ `tool_call.function.arguments` ä» JSON å­—ç¬¦ä¸²è½¬ä¸º dict
- è¿™ä¸ vLLM çš„ `_postprocess_messages()` è¡Œä¸ºä¸€è‡´ï¼ˆchat_utils.py:1425-1443ï¼‰
**æ–‡ä»¶ï¼š** server.py:373-396, 408
**æ—¥æœŸï¼š** 2026-01-11

### å·²çŸ¥é™åˆ¶

1. **ä¼ªæµå¼** - Transformers ä¸æ”¯æŒçœŸæµå¼ï¼Œç”Ÿæˆå®Œååˆ†å—å‘é€
2. **å•è¯·æ±‚** - ä¸æ”¯æŒå¹¶å‘å¤„ç†
3. **æ€§èƒ½è¾ƒæ…¢** - æ¯” vLLM æ…¢ 10-20 å€

### æµ‹è¯•çŠ¶æ€

| æµ‹è¯•é¡¹ | çŠ¶æ€ | è¯´æ˜ |
|--------|------|------|
| GET /v1/models | âœ… é€šè¿‡ | - |
| éæµå¼ chat completions | âœ… é€šè¿‡ | - |
| æµå¼ chat completions | âœ… é€šè¿‡ | - |
| å·¥å…·è°ƒç”¨ï¼ˆéæµå¼ï¼‰ | âœ… é€šè¿‡ | - |
| å·¥å…·è°ƒç”¨ï¼ˆæµå¼ï¼‰ | âœ… é€šè¿‡ | - |
| å¤šè½®å¯¹è¯ | âœ… é€šè¿‡ | å«å·¥å…·è°ƒç”¨å†å² |
| qwen-code 13å·¥å…·æµ‹è¯• | âœ… é€šè¿‡ | ä¿®å¤åå…¼å®¹ |

---

## æŠ€æœ¯æ¶æ„

### æ ¸å¿ƒç»„ä»¶

```
server.py
â”œâ”€â”€ FastAPI app                  # Web æ¡†æ¶
â”œâ”€â”€ Pydantic æ¨¡å‹                # OpenAI å…¼å®¹çš„æ•°æ®æ¨¡å‹
â”‚   â”œâ”€â”€ ChatCompletionRequest    # è¯·æ±‚æ¨¡å‹
â”‚   â”œâ”€â”€ ChatCompletionResponse   # å“åº”æ¨¡å‹
â”‚   â””â”€â”€ Tool / Function          # å·¥å…·å®šä¹‰æ¨¡å‹
â”œâ”€â”€ Tokenizer                    # åˆ†è¯å™¨
â”‚   â””â”€â”€ Chat template æ”¯æŒ       # è‡ªåŠ¨å¤„ç†å·¥å…·æ³¨å…¥
â”œâ”€â”€ Model Loader                 # æ¨¡å‹åŠ è½½
â”‚   â””â”€â”€ AutoModelForCausalLM     # Transformers æ¨¡å‹
â”œâ”€â”€ Tool Parser                  # å·¥å…·è§£æå™¨
â”‚   â””â”€â”€ StreamingXMLToolCallParser  # vLLM qwen3_xml
â”œâ”€â”€ Token Accumulator            # token ç´¯åŠ å™¨
â”‚   â””â”€â”€ é¿å…å¢é‡ decode ä¹±ç      # å®Œæ•´è§£ç åæå–å¢é‡
â”œâ”€â”€ Special Token Filter         # ç‰¹æ®Š token è¿‡æ»¤
â”‚   â””â”€â”€ è¿‡æ»¤ <|im_start|> ç­‰     # æ¸…ç†è¾“å‡º
â””â”€â”€ SSE Formatter               # æµå¼æ ¼å¼åŒ–
    â””â”€â”€ Server-Sent Events       # å…¼å®¹ OpenAI æ ¼å¼
```

### å…³é”®è®¾è®¡

#### 1. å¢é‡ Decode ç­–ç•¥

```python
class TokenAccumulator:
    """é¿å…å¢é‡ decode å¯¼è‡´çš„ä¹±ç """
    def __init__(self, tokenizer):
        self.tokenizer = tokenizer
        self.all_tokens = []
        self.last_decoded_len = 0

    def add_tokens(self, new_tokens):
        self.all_tokens.extend(new_tokens)
        full_text = self.tokenizer.decode(
            self.all_tokens,
            skip_special_tokens=True
        )
        delta_text = full_text[self.last_decoded_len:]
        self.last_decoded_len = len(full_text)
        return delta_text, full_text
```

#### 2. Pydantic é€’å½’è½¬æ¢

```python
def format_messages_for_template(messages: list[ChatMessage]) -> list[dict]:
    """é€’å½’è½¬æ¢ Pydantic æ¨¡å‹ä¸º dict"""
    formatted = []
    for msg in messages:
        # è½¬æ¢ä¸º dict
        if hasattr(msg, "model_dump"):
            msg_dict = msg.model_dump(exclude_none=True)
        else:
            msg_dict = dict(msg)

        # é€’å½’å¤„ç† tool_calls
        if msg_dict.get("tool_calls"):
            formatted_tool_calls = []
            for tc in msg_dict["tool_calls"]:
                if hasattr(tc, "model_dump"):
                    tc_dict = tc.model_dump(exclude_none=True)
                    # å¤„ç†åµŒå¥—çš„ function å­—æ®µ
                    if "function" in tc_dict and hasattr(tc_dict["function"], "model_dump"):
                        tc_dict["function"] = tc_dict["function"].model_dump(exclude_none=True)
                formatted_tool_calls.append(tc_dict)
            msg_dict["tool_calls"] = formatted_tool_calls

        formatted.append(msg_dict)
    return formatted
```

#### 3. å·¥å…·è°ƒç”¨æµå¼å¤„ç†

```python
async def generate_stream(request: ChatCompletionRequest):
    current_tool_call_id = None  # è·Ÿè¸ª ID

    for token_id in generated_ids.tolist():
        delta_text, full_text = accumulator.add_tokens([token_id])

        # è§£æå·¥å…·è°ƒç”¨
        result = tool_parser.parse_single_streaming_chunks(delta_text)

        if result.tool_calls:
            # ä¿æŒ tool_call_id ä¸€è‡´
            if current_tool_call_id is None:
                current_tool_call_id = f"chatcmpl-tool-{uuid.uuid4().hex}"

            # æ„å»º delta
            delta = {
                "content": result.content,
                "tool_calls": [{
                    "index": 0,
                    "id": current_tool_call_id,
                    "function": {
                        "name": result.tool_calls[0].function.name,
                        "arguments": result.tool_calls[0].function.arguments
                    }
                }]
            }
            yield format_sse(delta)
```

### æ–‡ä»¶ç»“æ„

```
FastAPI/
â””â”€â”€ qwen3coder/
    â”œâ”€â”€ server.py          # ä¸»æœåŠ¡å™¨
    â”œâ”€â”€ quick_start.sh     # å¿«é€Ÿå¯åŠ¨è„šæœ¬
    â”œâ”€â”€ test_client.py     # å®Œæ•´æµ‹è¯•å¥—ä»¶
    â””â”€â”€ README.md          # æœ¬æ–‡æ¡£
```

---

## å‚è€ƒèµ„æ–™

### ç›¸å…³æ–‡æ¡£

- [FastAPI å®˜æ–¹æ–‡æ¡£](https://fastapi.tiangolo.com/)
- [Transformers æ–‡æ¡£](https://huggingface.co/docs/transformers)
- [vLLM æ–‡æ¡£](https://docs.vllm.ai/)
- [OpenAI API æ–‡æ¡£](https://platform.openai.com/docs/api-reference)
- [Qwen æ¨¡å‹æ–‡æ¡£](https://huggingface.co/Qwen)

### æºç å‚è€ƒ

- **vLLM Tool Parser**: `/root/miniconda3/envs/qwen/lib/python3.12/site-packages/vllm/entrypoints/openai/tool_parsers/qwen3xml_tool_parser.py`
- **vLLM Qwen3 Model**: `/root/miniconda3/envs/qwen/lib/python3.12/site-packages/vllm/model_executor/models/qwen3.py`
- **vLLM Model Registry**: `/root/miniconda3/envs/qwen/lib/python3.12/site-packages/vllm/model_executor/models/registry.py`
- **vLLM Chat Utils**: `/root/miniconda3/envs/qwen/lib/python3.12/site-packages/vllm/entrypoints/chat_utils.py`

---

## è®¸å¯è¯

æœ¬é¡¹ç›®ä½¿ç”¨ä»¥ä¸‹å¼€æºç»„ä»¶ï¼š

| ç»„ä»¶ | è®¸å¯è¯ |
|------|--------|
| Qwen3-Coder-30B | Apache 2.0 |
| vLLM tool parser | Apache 2.0 |
| Transformers | Apache 2.0 |
| FastAPI | MIT |
| PyTorch | BSD-style |

---

**æœ€åæ›´æ–°ï¼š** 2026-01-11
**ç‰ˆæœ¬ï¼š** 1.0.0
**ç»´æŠ¤è€…ï¼š** Claude Code
